---
title: "Sepsis Survival Classification via Naives Bayes"
author: "Rex Manglicmot"
output: 
  github_document: 
    toc: yes
---
## Status: Continuing Working Document

Things still need to do/Questions:

* Concepts on Naive Bayes + Cite Sources
* color section for histogram
* Flush out insights for the exhibits
* Flush out intro

## Introduction
<center>

![](https://www.drugtargetreview.com/wp-content/uploads/sepsis-3.jpg)

</center>

Sepsis is a bacterial infection within the bloodstream and life-threatening. Any type of infection could cause seps such as catheter sites, wounds, burns, kidney, etc. The body is over actively working to fight the infection and as a result lead to septic shock (severe drop in blood pressure) that can lead to tissue damage, organ failure and even death. Symptoms include shivering, fast and shallow breathing, etc. 

According to the Center for Disease Control and Prevention, in a given year, 1 of 3 patients who have died within the hospital had sepsis during hospitalization, however, nearly 87% of sepsis cases starts before the patient goes to the hospital.^[https://www.cdc.gov/sepsis/what-is-sepsis.html#:~:text=In%20a%20typical%20year%3A,had%20sepsis%20during%20that%20hospitalization] It leaves hospitals institutions in a precarious situation. How do ultimately help a patient get better and not be subjected to malpractice suits? 

The purpose of this project is to classify if a patient with sepsis will due within a hospital. The goals of this study is to:

1. Better equip/train hospital staff for potential sepsis complications and thus create internal protocols, and also improve status quo.
2. Promote the uncovering of the underlying reasons why a patient dies within a hospital via academic research.


A special acknowledgement to the University of Irvine Data Repository for providing this dataset.^[https://archive.ics.uci.edu/ml/datasets/Sepsis+survival+minimal+clinical+records] 

The dataset contains 4 clinical features as follows:

* age(int): years: integer
* sex(binary): 0=male and 1=female
* episode_number (int): 
* hospital_outcome(boolean): 0=dead and 1=alive

## Loading the Libraries
```{r, message = FALSE}
#install.packages('hrbrthemes') #installed on 12/18/22
#install.packages('ggthemes') #installed on 12/18/22
#install.packages('ggdark') #installed on 12/8/22
#install.packages('colorspace') #installed on 12/8/22

library(tidyverse)
library(ggthemes)
library(ggplot2)
library(viridis)
library(colorspace)
library(scales)
library(naivebayes)
library(ROCR)
```

## Loading the Data
```{r}
#load the data
data_orig <- read.csv('Sepsis Survival.csv')

#view first 7 rows
head(data_orig, 7)
```

## Cleaning the Data
```{r}
#make a copy of the original dataset
data <- data_orig

#check for NA values
sum(is.na(data))

#change colnames
colnames(data) <- c('age', 'sex', 'epi', 'result')

#check for all values
uni <- lapply(data, unique)
uni

#check classes for all columns
unlist(lapply(data, class))

#change the sex, epi, and result columns 
data[,2:4] <- lapply(data[,2:4], as.factor)

#double check class but with a different function, sapply
sapply(data, class)
```
Let's explore

## Exploratory Data Analysis
```{r}
summary(data)
```

We see a few interesting insights from the summary function:

1. The mean is 62 and the median is 68 which means that Age as a symmetrical distribution.
2. Male and Females are almost the same, with slightly more males.
3. epi has more of the 1 category than the others. **Need to find more research about it and go back to the paper.**
4. More people with sepsis lived than died within the confines of the hospital. 

Let's visually check out the dataset. 
```{r, warning= FALSE, message=FALSE}
#create a histogram
ggplot(data, aes(x=age, fill= result)) +
  geom_histogram(alpha = .5, color = 'black') +
  facet_wrap(~result, scales= 'free_y') +
  scale_fill_discrete_qualitative() +
  theme_bw() +
  labs(title = 'Age Distribution by Those Who Died/Lived',
       subtitle = '0=Died and 1=Alive',
       y = 'Frequency',
       x = '')
```

We see two interesting insights:

* Due to the facet wrap, we see that there are more people who lived than died.
* Based on the Age distribution, we see that both populations were majority in the elderly age group and thus are right skewed. 

Let's see how they both compare.
```{r}
ggplot(data, aes(x=age, fill=result)) +
  geom_bar(position = 'dodge' ) +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

Visually, we see that there is big difference in terms of size. We see that there is obviously more females in study than there were males across all ages. 

Let's check out their density. 
```{r}
#create a density plot
ggplot(data, aes(x=age, fill= sex)) +
  geom_density(alpha = .5, color = 'black') +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

In terms of density, both male and females are overlapping and thus similar. 

```{r}
#create a violin plot
ggplot(data, aes(x=epi, y=age, fill=sex)) +
  geom_violin(alpha = .5, color='black') +
  scale_fill_discrete_qualitative() +
  theme_bw()
```


Insights:

* We see across both sexes and episodes that majority of the patients were the age 60+. What this could possibly mean is that Sepsis is likely to be caused in older age brackets and an insight we can gather is that when an older patient that is admitted to the hospital it could signal to the nursing staff that said patient may likely have a sepsis and precautions could be taken in the nursing unit. 

```{r}
ggplot(data, aes(x=epi, fill=result)) +
  geom_bar(alpha = .5, color='black', position = 'dodge', width=0.5,
           show.legend = TRUE) +
  facet_wrap(~ epi, scales = 'free') +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

In terms of Episodes there are a number of insights:

* In terms of sexes, there are clearly more females in all episodes. This insight  was expected given the disproportionate rate of males to females
* In terms of episodes, it is clear that most of the patients episodies were categorized majority in 1 and least in 5.


```{r}
#create a boxplot
ggplot(data, aes(x=result, y=age, fill=sex)) +
  geom_boxplot(alpha = .5, color = 'black') +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

Insights:

* In results, in those patients that died (0), we see that both male and females had outliers in the lower part of the box plot. Women age was slightly higher than males. 
*In the results, in those patients that lived (1), we see that the females had a better distrivtuion than that of males. 

```{r}
#create a barchart
ggplot(data, aes(x=result, fill=sex)) +
  geom_bar(alpha = .5, color='black', width = 0.5) +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

The default code to build a barchart is not helpful in drawing any insights. There is a need to scale each result (0 and 1) based on their respective y-axis metrics. THe code below does that.

```{r}
#create a better  barchart
ggplot(data, aes(x=result, fill=sex)) +
  geom_bar(alpha = .5, color='black', width = 0.5) +
  facet_wrap(~ result, scales = 'free') +
  scale_fill_discrete_qualitative() +
  theme_bw()
```

Insights:

* We see that there is about a ~2000 total patient difference between the results group 0 and 1.
* We see that the distribution of the sexes is slightly favorable towards the 0 group in both result groups.
* We can say that the study did a good job in getting a sample of sexes and results that are approximate. 

## Naives Bayes

Naive Bayes (NB) is a machine learning classifier based on Bayes' Theorem (describes the relationship of conditional probabilities). Further, NB uses the Bayes theorem which states we can find the probability of A given B. The core fo the theorem is is the assumption that each pair of features are independent of each other.^[https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c] 

<center>
![](https://miro.medium.com/max/750/1*tjcmj9cDQ-rHXAtxCu5bRQ.webp)

</center>
In the figure above, we are trying to find the probability of A given that B is true/has already occurred.

</center>
### Applications

* Sentiment Analysis. Identify positive and negative customer reviews.
* Spam filtering. Identify spam emails.
* Classify articles. Is the article, politics, healthcare, or oil & gas. 

The assumption is A and B are independent. 
</center>

### Pros

* Quick and easy to implement
* Useful for multiclass predictions as it is suited for categorical variables than numerical variables
* No iterations
* Requires small amount of training data and thus is NB is useful for small datasets.
* When independent predictors assumptions are true, NB performs better than other models.

### Cons

* Conditional independence assumption is not always true as there are cases where features are dependent on each other.
* Zero frequency problem, which is when a category variable is present in the test data set and not in the train dataset, the model will make a 0 probability. 


```{r}
#set seed to have reproducible results
set.seed(123)

#split data into a 70/30 split
ind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))

#split test and train
train <- data[ind==1, ]
test <- data[ind==2, ]

#build model
model <- naive_bayes(result~., data=train)
```


```{r}
#for numeric intergers we can get mean and sd for dead
train %>%
  dplyr::filter(result =='0') %>%
  summarise(mean(age), sd(age))

#for numeric intergers we can get mean and sd for alive
train %>%
  dplyr::filter(result =='1') %>%
  summarise(mean(age), sd(age))
```


```{r, message=FALSE, warning=FALSE}
#Predict
p <- predict(model, train, type = 'prob')
head(cbind(p, train))
```


```{r, message=FALSE, warning=FALSE}
#store prediction in an object
predict1 <- predict(model, train)

#create a confusion matrix
CM1 <- table(predicted = predict1, actual = train$result)

print(CM1)

train %>%
  summarize(accuracy = mean(result == 1))

train %>%
  summarize(accuracy = mean(result == 0))
```
Let's look diagonally:

* 0 patients were correctly predicted as classified as Dead (result = 0)
* 71551 patients were correctly predicted as classified as Alive (result = 1).

```{r, message=FALSE, warning=FALSE}
#calculate miscalculation error
1-sum(diag(CM1))/sum(CM1)
```

Misclassifications are about 7%.

Let's repeat for test set
```{r, message=FALSE, warning=FALSE}
#store prediction in an object
predict2 <- predict(model, test)

#create a confusion matrix
CM2 <- table(predicted = predict2, actual = test$result)

print(CM2)

test %>%
  summarize(accuracy = mean(result == 1))

test %>%
  summarize(accuracy = mean(result == 0))
```
Let's look diagonally:

* 0 patients were correctly predicted as classified as Dead (result = 0).
* 30548 patients were correctly predicted as classified as Alive (result = 1).

```{r, message=FALSE, warning=FALSE}
#calculate miscalculation error
1-sum(diag(CM2))/sum(CM2)
```
Misclassifications are about 7%.



## Limitations
One limitation is that although both miscalculation errors for the train and test dataset are low, about 7%, the model failed to predict accurately those who died within the hospital in both datasets. In fact, it the model got it all wrong! This can be due to prevalence; there is more 1 (Alive) than there is 0 (Dead) within the dataset. 

If our dataset in our train is biased then our results in out test will be biased as well. 

## Conclusion

## Appendix
```{r}
#plot the model
plot(model)
```


## Inspiration for this project